{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, link, content]\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def make_pg_num(num):\n",
    "    return 1 + 10 * (num - 1)\n",
    "\n",
    "def make_url(search, start_pg, end_pg):\n",
    "    urls = [f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search}&start={make_pg_num(i)}\" for i in range(start_pg, end_pg + 1)]\n",
    "    return urls\n",
    "\n",
    "def articles_crawler(url, headers):\n",
    "    try:\n",
    "        original_html = requests.get(url, headers=headers, timeout=5)\n",
    "        html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "        articles = html.select(\"div.group_news > ul.list_news > li div.news_area > a\")\n",
    "        return [(i.attrs['title'], i.attrs['href']) for i in articles]\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred when fetching content from {url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def news_contents_crawler(news_url, headers):\n",
    "    try:\n",
    "        news = requests.get(news_url, headers=headers, timeout=5)\n",
    "        news_html = BeautifulSoup(news.text, \"html.parser\")\n",
    "        content = news_html.find_all('p')\n",
    "        return \" \".join([c.get_text() for c in content])\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred when fetching content: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# User input\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "start_page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1:\"))\n",
    "end_page = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)3:\"))\n",
    "\n",
    "# Headers for request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "# URL generation\n",
    "urls = make_url(search, start_page, end_page)\n",
    "\n",
    "news_titles = []\n",
    "news_links = []\n",
    "news_contents = []\n",
    "\n",
    "# Crawling\n",
    "for url in urls:\n",
    "    for title, link in articles_crawler(url, headers):\n",
    "        news_titles.append(title)\n",
    "        news_links.append(link)\n",
    "        news_contents.append(news_contents_crawler(link, headers))\n",
    "        time.sleep(1)  # Delay between requests\n",
    "\n",
    "# DataFrame generation\n",
    "news_df = pd.DataFrame({'title': news_titles, 'link': news_links, 'content': news_contents})\n",
    "news_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "news_df.to_csv(f\"{search}_news.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
